# -*- coding: utf-8 -*-
"""Untitled149.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17-I6imkEC7dByR12K-_mLCZr8EKeYTgV
"""

class environment:
  def __init__(self, states = None):
    if states:
      self.states = states
    else:
      self.states = []
      for i in range(10):
        for j in range(10):
          self.states.append((i, j))
    self.state = (4, 4) 
    self.target = (8, 8)


  def reset(self):
    self.state = (np.random.randint(0, 10), np.random.randint(0, 10))


  def step(self, action):
    s_old = self.state
    if action == 'up':
      self.state = (s_old[0] + 1, s_old[1])
    elif action == 'down':
      self.state = (s_old[0] - 1, s_old[1])
    elif action == 'left':
      self.state = (s_old[0], s_old[1] - 1)
    else:
      self.state = (s_old[0], s_old[1] + 1)
    s = self.state
    r = - ((s[0] - s_old[0]) ** 2 + (s[1] - s_old[1]) ** 2) ** 0.5
    #Если агент выходит за пределы или достигает таргета
    if s not in self.states:
      r = -4
      s = 'out of boundary'
    elif s == self.target:
      r = 100
      s = 'target'
    return s, r

class agent:
  def __init__(self, env_states=None, policy=None, actions=None):
    if actions:
      self.actions = actions
    else:
      self.actions = ["up", "down", "left", "right"]
    if not policy: 
      self.policy = {}
      for state in env_states:
        self.policy[state] = {}
        for act in self.actions:
          self.policy[state][act] = 1 / 4
    else:
      self.policy = policy

import numpy as np

N = 200
M = 25
average_raward_for_session = []
env = environment()
agent_ = agent(env_states=env.states)


#Обучается на 50 итерациях
for iter in range(500):
  #Проигрывание N = 200 сессий
  history = []
  cumulative_rewars = []
  for epoch in range(N):
    R = 0
    epoch_history = []
    env.reset()
    state = env.state

    #Выбор начального действия
    action = None
    prob_choice = np.random.random()
    cumulative_prob = 0
    for act in agent_.actions:
      cumulative_prob += agent_.policy[state][act]
      if prob_choice <= cumulative_prob: action = act

    max_length_of_episode = 200
    length_of_episode = 0
    while True:
      if length_of_episode > max_length_of_episode: break
      state, r = env.step(action)
      if state == 'out of boundary' or state == 'target':
        R += r
        break

      #Выбор действия
      if np.random.random() < 0:
        action = np.random.choice(agent_.actions)
      else:
        action = None
        prob_choice = np.random.random()
        cumulative_prob = 0
        for act in agent_.actions:
          cumulative_prob += agent_.policy[state][act]
          if prob_choice <= cumulative_prob: action = act
      R += r
      epoch_history.append((state, action))
      length_of_episode += 1
    cumulative_rewars.append((R, epoch))
    history.append(epoch_history)
  
  #Выбор M = 25 лучших сессий
  cumulative_rewars = sorted(cumulative_rewars, key=lambda x: x[0])
  best_sessions = cumulative_rewars[-M:]
  best_state_action = [history[x[1]] for x in best_sessions]
  best_state_action = sum(best_state_action, [])
  #print(history[0])

  #Обновление стратегии
  env_states = env.states
  for state in env_states:
    actions_in_state = [pair[1] for pair in best_state_action if pair[0] == state]
    for act in agent_.actions:
      if len(actions_in_state) != 0:
        agent_.policy[state][act] = len(list(filter(lambda x: x==act, actions_in_state))) / len(actions_in_state)

  average_reward = sum([x[0] for x in cumulative_rewars]) / len(cumulative_rewars)
  average_raward_for_session.append(average_reward)
  if (iter + 1) % 25 == 4:
    print("done: ", (iter + 1) / 50, "with reward:", average_reward)

import matplotlib.pyplot as plt

plt.plot(average_raward_for_session)
plt.title("learning")
plt.xlabel('iteration')
plt.ylabel('average cumul reward')
None

np.random.choice(["up", "down", "left", "right"])
